\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={MITx: 6.86x Machine Learning with Python - From Linear Models to Deep Learning},
            colorlinks=true,
            linkcolor=cyan,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[margin=1in]{geometry}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother

\title{MITx: 6.86x Machine Learning with Python - From Linear Models to Deep
Learning}
\providecommand{\subtitle}[1]{}
\subtitle{Overview}
\author{}
\date{\vspace{-2.5em}December 2019}

\begin{document}
\maketitle

\begin{quote}
\textbf{Comment:} This overview has been written in R Markdown that
provides an authoring framework for data science. Learn more at
\url{https://bookdown.org/yihui/rmarkdown/}
\end{quote}

\thispagestyle{empty}
\newpage

\pagenumbering{gobble}
\pagenumbering{arabic}

\#MITx's MicroMasters Program in Statistics and Data Science\\
*** \textgreater{} \#\#\#From probability and statistics to data
analysis and machine learning, master the skills needed to solve complex
challenges with data.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{section}{%
\subsubsection{}\label{section}}

\begin{quote}
In september 2019. a great MIT's movement, to open their doors to the
whole world, has come to the field of Data Science. Anyone from the EdX
platform for online learning, showing interests for Data Science has
gotten an invitation to join. In the next year or so, the future
students were asked to pass 4 MIT exams and one Capstone exam, so they
could get the MIT's credentials in Data Science area. If they achieve
the goal, they could also apply for the second semester on MIT's campus
and finish \textbf{Masters of Statistics and Data Science} in a hybrid
generation of online and on campus students.
\end{quote}

\begin{quote}
Mastering the foundations of data science, statistics, and machine
learning using programming languages like \textbf{Python} and \textbf{R}
students are prepared for jobs like: Data Scientist, Data Analyst,
Business Intelligence Analyst, Systems Analyst, Data Engineer. More
about the program could be found
\href{https://micromasters.mit.edu/ds/}{\textbf{here}}.
\end{quote}

\#MITx: 6.86x Machine Learning with Python

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\begin{quote}
\hypertarget{an-in-depth-introduction-to-the-field-of-machine-learning-from-linear-models-to-deep-learning-and-reinforcement-learning-through-hands-on-python-projects.-course-4-of-4-in-the-mitx-micromasters-program-in-statistics-and-data-science.}{%
\subsubsection{An in-depth introduction to the field of machine
learning, from linear models to deep learning and reinforcement
learning, through hands-on Python projects. -- Course 4 of 4 in the MITx
MicroMasters program in Statistics and Data
Science.}\label{an-in-depth-introduction-to-the-field-of-machine-learning-from-linear-models-to-deep-learning-and-reinforcement-learning-through-hands-on-python-projects.-course-4-of-4-in-the-mitx-micromasters-program-in-statistics-and-data-science.}}
\end{quote}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{section-1}{%
\subsubsection{}\label{section-1}}

\begin{quote}
\href{https://www.edx.org/course/machine-learning-with-python-from-linear-models-to}{MITx:
6.86x} is advanced 14 week long instructor paced online course given by
MIT's Department of Electrical Engineering and Computer Science.
Prerequisites for this course are:
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  \href{https://www.edx.org/course/introduction-to-computer-science-and-programming-7}{MIT:
  6.00.1x} (Introduction to Computer Science and Programming Using
  Python) or proficiency in Python programming
\item
  \href{https://www.edx.org/course/probability-the-science-of-uncertainty-and-data}{MITx:
  6.431x} (Probability - The Science of Uncertainty and Data) or
  equivalent probability theory course
\item
  College-level single and multi-variable calculus
\item
  Vectors and matrices
\end{itemize}
\end{quote}

\begin{quote}
Course is presented in 6 Units, with 19 Lectures, 7 Homeworks and 5
Python Projects, through topics like:
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Representation, over-fitting, regularization, generalization, VC
  dimension
\item
  Clustering, classification, recommender problems, probabilistic
  modeling, reinforcement learning
\item
  On-line algorithms, support vector machines, and neural networks/deep
  learning
\end{itemize}
\end{quote}

\begin{quote}
where stidents are gaining knowledge about principles and algorithms for
turning training data into effective automated predictions. In it's
second edition, the course already has 50 thousands students enrolled.
\end{quote}

\newpage

\hypertarget{unit-0-course-overview-1-weeks}{%
\subsection{Unit 0: Course overview (1
weeks)}\label{unit-0-course-overview-1-weeks}}

\begin{quote}
\textbf{Brief Review of Vectors, Planes, and Optimization }
\end{quote}

\hypertarget{section-2}{%
\subsubsection{}\label{section-2}}

\begin{quote}
In this Unit, three videos (total 21:09 minutes) were presented showing
basic overview of:
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{vectors} in n-dimensional space (addition and subtraction of
  two vectors, norm of an vector, dot product of two vectors)
\end{itemize}
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  \textbf{plane definition} in n-dimensional spaces (used later in
  classification for decision boundary) as: a point \(x\) of
  n-dimensional space belongs to some plane if it satisfies the equation
  \(\theta \cdot x + \theta_{0} = 0\), where \(\theta\) is a vector
  perpendicular to that plane and \(\theta_{0}\) an offset of the plane
  from it's origin.
\item
  \textbf{loss function}
  (\(L(x, y; \theta) = \dfrac{1}{n} \sum_{i = 1}^{n} | \hat{y} - y_{i}|\)
  ), \textbf{gradient descent}
  (\(\hat{\theta} = \theta - \gamma \nabla_{\theta} L(x, y; \theta)\))
  and \textbf{chain rule} explained on some example function
\end{itemize}
\end{quote}

\begin{quote}
The homework for this Unit (total 32 questions) referred to the next
topics:
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Points and Vectors
\item
  Planes
\item
  Matrices
\item
  Probability Density Functions
\item
  Univariate Gaussians
\item
  Optimization and gradients
\end{enumerate}

\begin{quote}
Project 0 has initial Python setup and packages installation,
introduction to Numpy, Neural network exercise, introduction to ML
packages and one debugging exercise.
\end{quote}

\hypertarget{unit-1-linear-classifiers-and-generalizations-2-weeks}{%
\subsection{Unit 1: Linear Classifiers and Generalizations (2
weeks)}\label{unit-1-linear-classifiers-and-generalizations-2-weeks}}

\hypertarget{lecture-1.-introduction-to-machine-learning-total-video-3300-total-number-of-questions-16}{%
\subsubsection{Lecture 1. Introduction to Machine Learning (total video:
33:00, total number of questions:
16)}\label{lecture-1.-introduction-to-machine-learning-total-video-3300-total-number-of-questions-16}}

\begin{quote}
Objectives
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Understand the goal of machine learning from a movie recommender
  example
\item
  Understand elements of \textbf{supervised learning}, and the
  difference between the \textbf{training set} and the \textbf{test set}
\item
  Understand the difference of \textbf{classification} and
  \textbf{regression} - two representative kinds of supervised learning
\end{itemize}
\end{quote}

\begin{quote}
\textbf{Machine Learning} definition was introduced: Machine learning is
a discipline aims to design, understand, and apply computer programs
that learn from experience (i.e.~data) for the purpose of modelling,
prediction, and control. Main focus of this lecture is on the
prediction, explained through examples on the market value of a stock
measured as a function of time or in a self-driving vehicle, one can try
to predict whether a collision is about to happen, in a medical context,
one might predict the risk of acquiring or getting a recurrence for a
disease. There are other types of prediction such as someone liking a
movie or not, predicting the properties of an image or in machine
translation, translating one sentence from English to Spanish language
for example. \textbf{Supervised Learning} was introduced: machine
learning tasks are really provided in terms of examples of correct
behavior, like putting large number of images into an annotated
categories and then automating the process of finding the solution to a
task, that's illustrated by examples of images, as defining a set of
possible mappings depending of an image and a parameter outputting the
category of an image. A different value for these parameters will
specify a different mapping. The aim is out of all the possible
parameters to find the one that agrees the best with the given set of
examples.\\
A Concrete Example of a Supervised Learning Task: Movie Recommender
Problem. Each movie has a \textbf{feature vector} , \(x^{(i)}\), with
binary values answering specific questions about a given movie and a as
output, \(y^{(i)}\), +1 if one liked the movie and -1 if not. This is
how one gets a training set of feature vectors (examples) with the
associated \textbf{labels}. The task is to learn a mapping from those
vectors to the labels. This mapping is then tested on the test set
hoping that it will predict well the labels. Then naturally
\textbf{training error} and \textbf{test errors} are introduced as a
measure how good this mapping, a \textbf{classifier} -
\(h: \chi \to {-1, 1}\), performs. The problem can occur if the training
error is too small, that could mean possibility of overfiting the
classifier to the given data. In that case the test error would be to
high. Types of machine learning: - \textbf{Supervised learning} -
explicitly specify the correct behavior - \textbf{Unsupervised learning}
- strip the labels, only given the examples, model/find the
irregularities in how those examples vary. - \textbf{Semi-supervised
learning} - mix of supervised and unsupervised - \textbf{Active
learning} - the algorithm can itself ask for useful, additional examples
- \textbf{Transfer learning} - training the method based on one scenario
and applying the method in a different scenario - \textbf{Reinforcement
learning} - learning to control, learning to take actions in the world
so as to optimize some criteria
\end{quote}

\#3 Pictures

\hypertarget{lecture-2.-linear-classifier-and-perceptron-total-video-4528-total-number-of-questions-19}{%
\subsubsection{Lecture 2. Linear Classifier and Perceptron (total video:
45:28, total number of questions:
19)}\label{lecture-2.-linear-classifier-and-perceptron-total-video-4528-total-number-of-questions-19}}

\begin{quote}
Objectives
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Understand the concepts of Feature vectors and labels, Training set
  and Test set, Classifier, Training error, Test error, and the Set of
  classifiers
\item
  Understand the mathematical presentation of linear classifiers
\item
  Understand the intuitive and formal definition of linear separation
\item
  Understand the perceptron algorithm with and without offset
\end{itemize}
\end{quote}

\hypertarget{lecture-3-hinge-loss-margin-boundaries-and-regularization-total-video-2417-total-number-of-questions-10}{%
\subsubsection{Lecture 3 Hinge loss, Margin boundaries and
Regularization (total video: 24:17, total number of questions:
10)}\label{lecture-3-hinge-loss-margin-boundaries-and-regularization-total-video-2417-total-number-of-questions-10}}

\begin{quote}
Objectives
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Understand the need for maximizing the margin
\item
  Know how to pose linear classification as an optimization problem
\item
  Understand hinge loss, margin boundaries and regularization
\end{itemize}
\end{quote}

\hypertarget{homework-1-total-number-of-questions-36}{%
\subsubsection{Homework 1 (total number of questions:
36)}\label{homework-1-total-number-of-questions-36}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perceptron Mistakes
\item
  Perceptron Performance
\item
  Decision Boundaries
\item
  Feature Vectors
\end{enumerate}

\hypertarget{lecture-4.-linear-classification-and-generalization-total-video-2758-total-number-of-questions-11}{%
\subsubsection{Lecture 4. Linear Classification and Generalization
(total video: 27:58, total number of questions:
11)}\label{lecture-4.-linear-classification-and-generalization-total-video-2758-total-number-of-questions-11}}

\begin{quote}
Objectives
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Understanding optimization view of learning
\item
  Optimization algorithms - gradient descent, stochastic gradient
  descent, quadratic program
\end{itemize}
\end{quote}

\hypertarget{homework-2-total-number-of-questions-12}{%
\subsubsection{Homework 2 (total number of questions:
12)}\label{homework-2-total-number-of-questions-12}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Linear Support Vector Machines
\item
  Passive-aggressive algorithm
\item
  Perceptron Updates
\end{enumerate}

\hypertarget{project-1-automatic-review-analyzer-number-of-coding-questions-9-number-of-additional-questions-30}{%
\subsubsection{Project 1: Automatic Review Analyzer (number of coding
questions: 9, number of additional questions
30)}\label{project-1-automatic-review-analyzer-number-of-coding-questions-9-number-of-additional-questions-30}}

\begin{quote}
The goal of this project is to design a classifier to use for sentiment
analysis of product reviews. The training set consists of reviews
written by Amazon customers for various food products. The reviews,
originally given on a 5 point scale, have been adjusted to a +1 or -1
scale, representing a positive or negative review, respectively.
\end{quote}

\begin{quote}
Python solutions on the given topics were required:
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Hinge Loss
\item
  Perceptron Algorithm
\item
  Algorithm Discussion
\item
  Classification and Accuracy
\item
  Parameter Tuning
\item
  Feature Engineering
\end{enumerate}

\hypertarget{unit-2-nonlinear-classification-linear-regression-collaborative-filtering-2-weeks}{%
\subsection{Unit 2 Nonlinear Classification, Linear regression,
Collaborative Filtering (2
weeks)}\label{unit-2-nonlinear-classification-linear-regression-collaborative-filtering-2-weeks}}

\#\#\#Lecture 5. Linear Regression (total video: 50:46, total number of
questions: 10)

\begin{quote}
Objectives
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Write the training error as least squares criterion for linear
  regression
\item
  Use stochastic gradient descent for fitting linear regression models
\item
  Solve closed-form linear regression solution
\item
  Identify regularization term and how it changes the solution,
  generalization
\end{itemize}
\end{quote}

\hypertarget{lecture-6.-nonlinear-classification-kernels-total-video-4913-total-number-of-questions-13}{%
\subsubsection{Lecture 6. Nonlinear Classification: Kernels (total
video: 49:13, total number of questions:
13)}\label{lecture-6.-nonlinear-classification-kernels-total-video-4913-total-number-of-questions-13}}

\begin{quote}
Objectives
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Non-linear classifiers from feature maps
\item
  Moving from coordinate parameterization to weighting examples
\item
  Kernel functions induced from feature maps
\item
  Kernel perceptron, kernel linear regression
\item
  Properties of kernel functions
\end{itemize}
\end{quote}

\hypertarget{lecture-7.-recommender-systems-total-video-3641-total-number-of-questions-9}{%
\subsubsection{Lecture 7. Recommender Systems (total video: 36:41, total
number of questions:
9)}\label{lecture-7.-recommender-systems-total-video-3641-total-number-of-questions-9}}

\begin{quote}
Objectives
\end{quote}

\begin{quote}
\begin{itemize}
\tightlist
\item
  Understand the problem definition and assumptions of recommender
  systems
\item
  Understand the impact of similarity measures in the K-Nearest Neighbor
  method
\item
  Understand the need to impose the low rank assumption in collaborative
  filtering
\item
  Iteratively find values of \(U\) and \(V\) (given \(X = UV^T\) ) in
  collaborative filtering
\end{itemize}
\end{quote}

\hypertarget{homework-3-total-number-of-questions-26}{%
\subsubsection{Homework 3 (total number of questions:
26)}\label{homework-3-total-number-of-questions-26}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Collaborative Filtering, Kernels, Linear Regression
\item
  Feature Vectors Transformation
\item
  Kernels
\item
  Linear Regression and Regularization
\end{enumerate}

\hypertarget{project-2-digit-recognition-part-1-number-of-coding-questions-11-number-of-additional-questions-20}{%
\subsubsection{Project 2: Digit recognition (Part 1) (number of coding
questions: 11, number of additional questions
20)}\label{project-2-digit-recognition-part-1-number-of-coding-questions-11-number-of-additional-questions-20}}

\begin{quote}
The MNIST database contains binary images of handwritten digits commonly
used to train image processing systems. The digits were collected from
among Census Bureau employees and high school students. The database
contains 60,000 training digits and 10,000 testing digits, all of which
have been size-normalized and centered in a fixed-size image of 28 × 28
pixels. Many methods have been tested with this dataset and in this
project, one will get a chance to experiment with the task of
classifying these images into the correct digit using some of the
methods you have learned so far.
\end{quote}

\begin{quote}
Python solutions on the given topics were required:
\end{quote}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Support Vector Machine
\item
  Multinomial (Softmax) Regression and Gradient Descent
\item
  Classification Using Manually Crafted Features
\item
  Dimensionality Reduction Using PCA
\item
  Cubic Features
\item
  Kernel Methods
\end{enumerate}

\hypertarget{unit-3-neural-networks-2.5-weeks}{%
\subsection{Unit 3 Neural networks (2.5
weeks)}\label{unit-3-neural-networks-2.5-weeks}}

\hypertarget{lecture-8.-introduction-to-feedforward-neural-networks}{%
\subsubsection{Lecture 8. Introduction to Feedforward Neural
Networks}\label{lecture-8.-introduction-to-feedforward-neural-networks}}

\emph{(total video: 36:08, total number of questions: 17)}

\end{document}
